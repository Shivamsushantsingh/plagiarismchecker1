{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONhvlTEwBsO0kGfFDQG5T/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99Cb1NO1cj9T","executionInfo":{"status":"ok","timestamp":1742405578304,"user_tz":-330,"elapsed":4158,"user":{"displayName":"Shivam Kumar Singh","userId":"08243419528993625701"}},"outputId":"c660bc8f-02ae-4ce6-8ee7-04a5d93268d9"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import os\n","import string\n","import re\n","import nltk\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity\n","import pickle\n","\n","# Download NLTK resources (run once)\n","nltk.download('punkt')\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","source":["!unzip xyzdataset.zip -d xyzdataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nf4EAj56dOlI","executionInfo":{"status":"ok","timestamp":1742405583935,"user_tz":-330,"elapsed":159,"user":{"displayName":"Shivam Kumar Singh","userId":"08243419528993625701"}},"outputId":"06d9ff8a-9864-454a-a69a-169ac23db305"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  xyzdataset.zip\n","  inflating: xyzdataset/train_snli.txt  \n"]}]},{"cell_type":"code","source":["# 2. Function to read .txt files\n","def read_txt(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        return file.read()\n","\n","# 3. Load and split the dataset from xyz_dataset.txt\n","dataset_path = '/content/xyzdataset/train_snli.txt'  # Adjust path if needed\n","documents = []\n","labels = []\n","\n","# Read the single file and split into documents and labels\n","if os.path.exists(dataset_path):\n","    with open(dataset_path, 'r', encoding='utf-8') as file:\n","        lines = [line.strip() for line in file.readlines() if line.strip()]\n","\n","    for line in lines:\n","        parts = line.split('\\t')  # Split by tab\n","        if len(parts) >= 3:  # text1, text2, label\n","            text = \"\\t\".join(parts[:-1])  # Combine text1 and text2 with tab\n","            label = int(parts[-1])  # Last part is 0 or 1\n","            documents.append(text)\n","            labels.append(label)\n","else:\n","    raise FileNotFoundError(f\"{dataset_path} not found!\")\n","\n","# Split: 80% train (8000 lines), 20% test (2000 lines)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    documents, labels, test_size=0.2, random_state=42, stratify=labels\n",")\n","\n","# 4. Data Preprocessing Function\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = str(text).lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Join tokens back to text\n","    return ' '.join(tokens)\n","\n","# 5. Preprocess all documents\n","processed_docs_train = [preprocess_text(doc) for doc in X_train]\n","processed_docs_test = [preprocess_text(doc) for doc in X_test]\n","\n","# 7. Create TF-IDF Vectorizer and fit on training data\n","vectorizer = TfidfVectorizer()\n","tfidf_train = vectorizer.fit_transform(processed_docs_train)\n","tfidf_test = vectorizer.transform(processed_docs_test)\n","\n","# 8. Plagiarism Checker Class\n","class PlagiarismChecker:\n","    def __init__(self, vectorizer, tfidf_matrix, threshold=0.8):\n","        self.vectorizer = vectorizer\n","        self.tfidf_matrix = tfidf_matrix\n","        self.threshold = threshold\n","\n","    def check_plagiarism(self, text):\n","        # Preprocess input text\n","        processed_text = preprocess_text(text)\n","\n","        # Transform text to TF-IDF\n","        text_tfidf = self.vectorizer.transform([processed_text])\n","\n","        # Calculate cosine similarity with all documents\n","        similarities = cosine_similarity(text_tfidf, self.tfidf_matrix)[0]\n","\n","        # Find maximum similarity\n","        max_similarity = np.max(similarities)\n","\n","        # Return result\n","        return {\n","            'is_plagiarized': max_similarity >= self.threshold,\n","            'similarity_score': max_similarity,\n","            'most_similar_doc_index': np.argmax(similarities)\n","        }\n","\n","# 9. Training and Evaluation\n","checker = PlagiarismChecker(vectorizer, tfidf_train)\n","\n","# Test the model\n","y_pred = []\n","for doc in processed_docs_test:\n","    result = checker.check_plagiarism(doc)\n","    y_pred.append(1 if result['is_plagiarized'] else 0)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# 10. Working Demo Function\n","def demo_plagiarism_checker(text):\n","    result = checker.check_plagiarism(text)\n","    print(\"\\nPlagiarism Check Results:\")\n","    print(f\"Input Text: {text}\")\n","    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n","    print(f\"Plagiarism Detected: {result['is_plagiarized']}\")\n","    print(f\"Most Similar Document Index: {result['most_similar_doc_index']}\")\n","    print(f\"Most Similar Document: {X_train[result['most_similar_doc_index']]}\")\n","\n","# 11. Test the demo with sample texts\n","print(\"\\n=== Demo Tests ===\")\n","test_texts = [\n","    \"This is a test document about machine learning\",\n","    \"Completely unique and original content\",\n","    \"Machine learning powers modern technology\"\n","]\n","\n","for test_text in test_texts:\n","    demo_plagiarism_checker(test_text)\n","\n","# 12. Dataset Statistics\n","print(\"\\n=== xyz_dataset Statistics ===\")\n","print(f\"Total documents: {len(documents)}\")\n","print(f\"Number of training documents: {len(X_train)}\")\n","print(f\"Number of test documents: {len(X_test)}\")\n","print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n","print(f\"Plagiarized documents: {sum(labels)}\")\n","print(f\"Original documents: {len(labels) - sum(labels)}\")\n","\n","# Optional: Save the model\n","with open('plagiarism_checker.pkl', 'wb') as f:\n","    pickle.dump({\n","        'vectorizer': vectorizer,\n","        'tfidf_matrix': tfidf_train,\n","        'threshold': checker.threshold\n","    }, f)\n","print(\"\\nModel saved as 'plagiarism_checker.pkl'\")"],"metadata":{"id":"gaionyFJdbNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xI60clQIdeNg"},"execution_count":null,"outputs":[]}]}